{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e95178b-6f83-4b2e-bdb9-1accec5a9369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (9912, 4)\n",
      "Test shape: (495, 4)\n",
      "Preference data shape: (1762, 4)\n",
      "\n",
      "Cleaned test_df columns: Index(['prompt', 'essay', 'evaluation', 'band'], dtype='object')\n",
      "\n",
      ") ===ssay 0 (Gold Band 7.5\n",
      "[Gemini Eval Done ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=400) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Local LLM Eval Done ]\n",
      "\n",
      ") ===ssay 1 (Gold Band 6\n",
      "[Gemini Eval Done ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=400) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Local LLM Eval Done ]\n",
      "\n",
      ") ===ssay 2 (Gold Band <4\n",
      "[Gemini Eval Done ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=400) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Local LLM Eval Done ]\n",
      "\n",
      ") ===ssay 3 (Gold Band 6\n",
      "[Gemini Eval Done ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=400) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Local LLM Eval Done ]\n",
      "\n",
      ") ===ssay 4 (Gold Band 6\n",
      "[Gemini Eval Done ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=400) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Local LLM Eval Done ]\n",
      "\n",
      " Results saved to: first_version_results.csv\n",
      "\n",
      "[Future Scope]\n",
      " Fine-tune open-source LLMs with train.csv\n",
      " Evaluate performance on test.csv\n",
      " Use preference_data.csv for RLHF training\n",
      " Build web app for deployment (Streamlit/Flask)\n"
     ]
    }
   ],
   "source": [
    "# IELTS Essay Evaluation - First Version\n",
    "# Datasets: train.csv, test.csv, preference_data.csv\n",
    "# Models: Google Gemini API + Open-source HuggingFace LLM\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# 1. Load Datasets\n",
    "\n",
    "\n",
    "TRAIN_PATH = \"/Users/kalpanapullagura/Downloads/Automated-IELTS-essay-evaluation-master/data/train.csv\"\n",
    "TEST_PATH = \"/Users/kalpanapullagura/Downloads/Automated-IELTS-essay-evaluation-master/data/test.csv\"\n",
    "PREF_PATH = \"/Users/kalpanapullagura/Downloads/Automated-IELTS-essay-evaluation-master/data/preference_data.csv\"\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH, lineterminator=\"\\n\")\n",
    "test_df = pd.read_csv(TEST_PATH, lineterminator=\"\\n\")\n",
    "pref_df = pd.read_csv(PREF_PATH, lineterminator=\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "train_df.columns = train_df.columns.str.strip()\n",
    "test_df.columns = test_df.columns.str.strip()\n",
    "pref_df.columns = pref_df.columns.str.strip()\n",
    "\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "print(\"Preference data shape:\", pref_df.shape)\n",
    "\n",
    "print(\"\\nCleaned test_df columns:\", test_df.columns)\n",
    "\n",
    "\n",
    "# 2. Define Evaluation Prompt\n",
    "\n",
    "evaluation_prompt = \"\"\"\n",
    "The score of the essay below is **{}**. Evaluate this essay that aligns with the score provided.\n",
    "\n",
    "## Task Achievement:\n",
    "- How well the candidate addresses the task.\n",
    "\n",
    "## Coherence and Cohesion:\n",
    "- Logical flow, transitions, structure.\n",
    "\n",
    "## Lexical Resource:\n",
    "- Vocabulary range, mistakes, and suggestions.\n",
    "\n",
    "## Grammatical Range and Accuracy:\n",
    "- Grammar range and errors.\n",
    "\n",
    "## Overall Band Score:\n",
    "- Suggested band score.\n",
    "\n",
    "## Essay Prompt:\n",
    "{}\n",
    "\n",
    "## Essay:\n",
    "{}\n",
    "\n",
    "## Evaluation:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# 3A. Gemini API Evaluation\n",
    "\n",
    "def evaluate_with_gemini(prompt, essay, band):\n",
    "    import google.generativeai as genai\n",
    "    load_dotenv()\n",
    "    genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "    model = genai.GenerativeModel(\"models/gemini-1.5-flash-latest\") \n",
    "    full_prompt = evaluation_prompt.format(band, prompt, essay)\n",
    "    response = model.generate_content(full_prompt)\n",
    "    return response.text.strip()\n",
    "\n",
    "\n",
    "\n",
    "# 3B. Open-source LLM Evaluation (HuggingFace)\n",
    "\n",
    "def evaluate_with_local_llm(prompt, essay, band):\n",
    "    from transformers import pipeline\n",
    "    #  First version = small model (gpt2) just to demonstrate pipeline\n",
    "    # Future work = replace with Mistral/LLaMA-2 and fine-tune\n",
    "    local_model = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "    full_prompt = evaluation_prompt.format(band, prompt, essay)\n",
    "    output = local_model(full_prompt, max_length=400, do_sample=True)\n",
    "    return output[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "# 4. Run Evaluation on Samples\n",
    "\n",
    "results = []\n",
    "\n",
    "for idx, row in test_df.head(5).iterrows():   # run only 5 for demo\n",
    "    essay_prompt = row[\"prompt\"]\n",
    "    essay_text = row[\"essay\"]\n",
    "    essay_band = row[\"band\"]\n",
    "\n",
    "    print(f\"\\n=== Essay {idx} (Gold Band {essay_band}) ===\")\n",
    "\n",
    "    # Gemini evaluation\n",
    "    try:\n",
    "        gemini_eval = evaluate_with_gemini(essay_prompt, essay_text, essay_band)\n",
    "        print(\"[Gemini Eval Done ]\")\n",
    "    except Exception as e:\n",
    "        gemini_eval = f\"Gemini error: {e}\"\n",
    "        print(\"[Gemini Eval Failed ]\")\n",
    "\n",
    "    # Local LLM evaluation\n",
    "    try:\n",
    "        local_eval = evaluate_with_local_llm(essay_prompt, essay_text, essay_band)\n",
    "        print(\"[Local LLM Eval Done ]\")\n",
    "    except Exception as e:\n",
    "        local_eval = f\"Local LLM error: {e}\"\n",
    "        print(\"[Local LLM Eval Failed ]\")\n",
    "\n",
    "    results.append({\n",
    "        \"prompt\": essay_prompt,\n",
    "        \"essay\": essay_text,\n",
    "        \"gold_band\": essay_band,\n",
    "        \"gemini_evaluation\": gemini_eval,\n",
    "        \"local_llm_evaluation\": local_eval\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "# 5. Save Results\n",
    "\n",
    "out_path = \"first_version_results.csv\"\n",
    "pd.DataFrame(results).to_csv(out_path, index=False)\n",
    "print(\"\\n Results saved to:\", out_path)\n",
    "\n",
    "\n",
    "\n",
    "# 6. Future Scope (Preference Data)\n",
    "\n",
    "\n",
    "print(\"\\n[Future Scope]\")\n",
    "print(\" Fine-tune open-source LLMs with train.csv\")\n",
    "print(\" Evaluate performance on test.csv\")\n",
    "print(\" Use preference_data.csv for RLHF training\")\n",
    "print(\" Build web app for deployment (Streamlit/Flask)\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "143b9c1f-0169-4ebf-b22c-0dcf29c447af",
   "metadata": {},
   "source": [
    "1.What We Have Built So Far\n",
    "We have built a system that takes an IELTS essay and sends it to two different types of AI:\n",
    "Google’s Gemini API (a large commercial model).\n",
    "GPT-2 (a small, open-source model that runs on our computer).\n",
    "Both models give their evaluation of the essay, and we save the results side by side in a CSV file. This way, we can directly compare how the two models judge the same essay.\n",
    "\n",
    "2.Why This Is Useful\n",
    "This is useful because we are not just testing one AI—we are comparing two different approaches: a big commercial model and a small local model. By doing this, we can see how their answers differ and think about which one is more suitable for IELTS scoring. We also have the option to adjust the local model to better fit IELTS requirements, which is something we can’t do with a commercial model.\n",
    "\n",
    "3.Future Scope \n",
    "Our plan is to:\n",
    "Train an open-source model with IELTS essay data so it becomes better at scoring essays.\n",
    "Improve the way the model gives feedback so it’s clearer and easier to understand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
